{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Architecture"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "541afb61ad5bd355"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:17:08.621390Z",
     "start_time": "2023-11-05T12:17:08.615544Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Loading dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from src.data.make_dataset import TextDataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:17:09.051667Z",
     "start_time": "2023-11-05T12:17:09.042273Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df = pickle.load(open('../data/interim/text_dataset.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:17:09.378729Z",
     "start_time": "2023-11-05T12:17:09.229400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                toxic  \\\n5   I'm not gonna have a child... ...with the same...   \n6   They're all laughing at us, so we'll kick your...   \n7     Maine was very short on black people back then.   \n11  So now their spirits are cursed, walking back ...   \n13               Come on, Cal, leave that shit alone.   \n\n                                               normal  toxic_reduction  \n5   I'm not going to breed kids with a genetic dis...         0.915109  \n6             they're laughing at us. We'll show you.         0.999361  \n7              there wasn't much black in Maine then.         0.814971  \n11  their souls are cursed, they guard the paths, ...         0.698517  \n13                         come on, Cal, put it down.         0.999357  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>toxic</th>\n      <th>normal</th>\n      <th>toxic_reduction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>I'm not gonna have a child... ...with the same...</td>\n      <td>I'm not going to breed kids with a genetic dis...</td>\n      <td>0.915109</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>They're all laughing at us, so we'll kick your...</td>\n      <td>they're laughing at us. We'll show you.</td>\n      <td>0.999361</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Maine was very short on black people back then.</td>\n      <td>there wasn't much black in Maine then.</td>\n      <td>0.814971</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>So now their spirits are cursed, walking back ...</td>\n      <td>their souls are cursed, they guard the paths, ...</td>\n      <td>0.698517</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Come on, Cal, leave that shit alone.</td>\n      <td>come on, Cal, put it down.</td>\n      <td>0.999357</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:17:09.431364Z",
     "start_time": "2023-11-05T12:17:09.422440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Cleaning text...\n",
      "2. Tokenizing text...\n",
      "3. Removing stopwords...\n",
      "4. Lemmatizing text...\n",
      "Transfomed data sample:\n",
      "                                                    toxic  \\\n",
      "508062  [claimed, forced, avoid, church, wanted, visit...   \n",
      "137379                            [useless, even, manage]   \n",
      "24365          [point, like, right, chick, really, crazy]   \n",
      "147195  [wondering, whether, really, would, cut, throa...   \n",
      "125724                                             [kick]   \n",
      "\n",
      "                                                   normal  toxic_reduction  \n",
      "508062  [said, felt, compelled, abstain, church, becam...         0.772285  \n",
      "137379                                        [use, even]         0.998066  \n",
      "24365             [case, like, right, cat, really, crazy]         0.925730  \n",
      "147195                  [wonder, able, cut, said, magrat]         0.942295  \n",
      "125724                                             [kick]         0.923199  \n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "from src.data.transforms import apply_transforms\n",
    "\n",
    "df_transformed = TextDataset(df=df.data.copy())\n",
    "apply_transforms(df_transformed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:18:18.748853Z",
     "start_time": "2023-11-05T12:17:09.638802Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                toxic  \\\n5   [gon, na, child, genetic, disorder, gon, na, d...   \n6                             [laughing, u, kick, as]   \n7                 [maine, short, black, people, back]   \n11  [spirit, cursed, walking, back, road, waterway...   \n13                    [come, cal, leave, shit, alone]   \n\n                                               normal  toxic_reduction  \n5   [going, breed, kid, genetic, disorder, make, die]         0.915109  \n6                                 [laughing, u, show]         0.999361  \n7                                [much, black, maine]         0.814971  \n11  [soul, cursed, guard, path, say, encounter, un...         0.698517  \n13                                   [come, cal, put]         0.999357  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>toxic</th>\n      <th>normal</th>\n      <th>toxic_reduction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>[gon, na, child, genetic, disorder, gon, na, d...</td>\n      <td>[going, breed, kid, genetic, disorder, make, die]</td>\n      <td>0.915109</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[laughing, u, kick, as]</td>\n      <td>[laughing, u, show]</td>\n      <td>0.999361</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[maine, short, black, people, back]</td>\n      <td>[much, black, maine]</td>\n      <td>0.814971</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>[spirit, cursed, walking, back, road, waterway...</td>\n      <td>[soul, cursed, guard, path, say, encounter, un...</td>\n      <td>0.698517</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>[come, cal, leave, shit, alone]</td>\n      <td>[come, cal, put]</td>\n      <td>0.999357</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:18:18.756706Z",
     "start_time": "2023-11-05T12:18:18.753302Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Loading Toxic Words set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "toxic_words_set = pickle.load(open('../data/interim/toxic_words.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T08:58:19.718537Z",
     "start_time": "2023-11-05T08:58:19.713630Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Loading Toxicity Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from src.models.config import CLF_PATH, DEVICE\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(CLF_PATH)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(CLF_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:25:23.967111Z",
     "start_time": "2023-11-05T09:25:22.377416Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:25:24.876915Z",
     "start_time": "2023-11-05T09:25:24.870343Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_toxicity(prompts):\n",
    "    with torch.inference_mode():\n",
    "        inputs = tokenizer(prompts, return_tensors='pt', padding=True).to(DEVICE)\n",
    "\n",
    "        out = torch.softmax(model(**inputs).logits, -1)[:, 1].cpu().numpy()\n",
    "\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:37:11.842231Z",
     "start_time": "2023-11-05T09:37:11.829539Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Trying different architectures"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Baseline Model: removing toxic words from sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def apply_remove_model(raw_sent, toxic_words):\n",
    "    words = raw_sent.split()\n",
    "    result = []\n",
    "\n",
    "    for word in words:\n",
    "        result.append(word) if word not in toxic_words else None\n",
    "\n",
    "    return ' '.join(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:37:12.274150Z",
     "start_time": "2023-11-05T09:37:12.267702Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Applying for random sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def test_remove_model(n_samples, toxic_words):\n",
    "    i = 1\n",
    "    for index, row in df.data.sample(n=n_samples).iterrows():\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"SENTENCE {i}/{n_samples}\")\n",
    "        i += 1\n",
    "\n",
    "        orig_sent = row['toxic']\n",
    "        target_sent = row['normal']\n",
    "        result_sent = apply_remove_model(orig_sent, toxic_words)\n",
    "\n",
    "        toxic_scores = get_toxicity([orig_sent, result_sent, target_sent])\n",
    "\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Original sentence:\", orig_sent)\n",
    "        print(\"Original toxicity:\", toxic_scores[0])\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Result sentence:\", result_sent)\n",
    "        print(\"Result toxicity:\", toxic_scores[1])\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Target sentence:\", target_sent)\n",
    "        print(\"Target toxicity:\", toxic_scores[2])\n",
    "        print(\"-\" * 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:37:12.853897Z",
     "start_time": "2023-11-05T09:37:12.846713Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "SENTENCE 1/10\n",
      "----------\n",
      "Original sentence: No matter what side you're on. You're always the looser.\n",
      "Original toxicity: 0.97803134\n",
      "----------\n",
      "Result sentence: No matter what side you're on. You're always the looser.\n",
      "Result toxicity: 0.97803134\n",
      "----------\n",
      "Target sentence: whatever you're on, you'll always lose.\n",
      "Target toxicity: 0.00016438692\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 2/10\n",
      "----------\n",
      "Original sentence: Two: why are youfucking llona then?\n",
      "Original toxicity: 0.9957242\n",
      "----------\n",
      "Result sentence: Two: why are then?\n",
      "Result toxicity: 4.0355728e-05\n",
      "----------\n",
      "Target sentence: second: Why are you sleeping with Ilons?\n",
      "Target toxicity: 9.117348e-05\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 3/10\n",
      "----------\n",
      "Original sentence: Maybe buy my fathead.\n",
      "Original toxicity: 0.9798124\n",
      "----------\n",
      "Result sentence: Maybe buy my fathead.\n",
      "Result toxicity: 0.9798124\n",
      "----------\n",
      "Target sentence: maybe buy my poster.\n",
      "Target toxicity: 4.673778e-05\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 4/10\n",
      "----------\n",
      "Original sentence: but if you could choose who lives and who dies...... you would become a monster.\n",
      "Original toxicity: 0.92008376\n",
      "----------\n",
      "Result sentence: but if you could choose who lives and who dies...... you would become a monster.\n",
      "Result toxicity: 0.92008376\n",
      "----------\n",
      "Target sentence: But if you could choose, Doctor, if you could decide who lives and who dies... ...that would make you a monster.\n",
      "Target toxicity: 0.15986441\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 5/10\n",
      "----------\n",
      "Original sentence: \"And stop burgling his bloody office.\"\n",
      "Original toxicity: 0.9927222\n",
      "----------\n",
      "Result sentence: \"And stop burgling his bloody office.\"\n",
      "Result toxicity: 0.9927222\n",
      "----------\n",
      "Target sentence: and stop wasting his office! \"For God's sake.\n",
      "Target toxicity: 0.0010882143\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 6/10\n",
      "----------\n",
      "Original sentence: do you mind your head, monster?\n",
      "Original toxicity: 0.99711096\n",
      "----------\n",
      "Result sentence: do you mind your head, monster?\n",
      "Result toxicity: 0.99711096\n",
      "----------\n",
      "Target sentence: Are you looking for the head, Monster?\n",
      "Target toxicity: 0.061770614\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 7/10\n",
      "----------\n",
      "Original sentence: To the inmates, you're Jensen Ames, the new grease monkey.\n",
      "Original toxicity: 0.96805364\n",
      "----------\n",
      "Result sentence: To the inmates, you're Jensen Ames, the new grease monkey.\n",
      "Result toxicity: 0.96805364\n",
      "----------\n",
      "Target sentence: for the other prisoners, you're Jensen Ames, a new mechanic.\n",
      "Target toxicity: 0.0001260747\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 8/10\n",
      "----------\n",
      "Original sentence: I want him hunted down.\n",
      "Original toxicity: 0.99194044\n",
      "----------\n",
      "Result sentence: I want him hunted down.\n",
      "Result toxicity: 0.99194044\n",
      "----------\n",
      "Target sentence: I want to track him down.\n",
      "Target toxicity: 0.00028817312\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 9/10\n",
      "----------\n",
      "Original sentence: Just break into Rona's apartment and put her stupid journal somewhere.\n",
      "Original toxicity: 0.9989292\n",
      "----------\n",
      "Result sentence: Just break into Rona's apartment and put her stupid journal somewhere.\n",
      "Result toxicity: 0.9989292\n",
      "----------\n",
      "Target sentence: just break into Ronnie's room and throw her a diary.\n",
      "Target toxicity: 0.0043020854\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 10/10\n",
      "----------\n",
      "Original sentence: Say, if I remember correctly, you owe me a pot of gold, sucker.\n",
      "Original toxicity: 0.99635375\n",
      "----------\n",
      "Result sentence: Say, if I remember correctly, you owe me a pot of gold, sucker.\n",
      "Result toxicity: 0.99635375\n",
      "----------\n",
      "Target sentence: well, if I remember correctly, you owe me a pot of gold.\n",
      "Target toxicity: 8.549631e-05\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_remove_model(10, toxic_words_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:37:13.480814Z",
     "start_time": "2023-11-05T09:37:13.065915Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, my set of toxic words doesn't work and it is expected. Because my set is just simple difference of words between toxic and nontoxic sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Trying well-prepared set of toxic and negatice words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "with open('../data/external/toxic_words.txt', 'r') as f:\n",
    "    toxic_words_prepared = list(map(lambda x: x.strip(), f.readlines()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:34:26.903294Z",
     "start_time": "2023-11-05T09:34:26.902615Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "with open('../data/external/negative-words.txt', 'r') as f:\n",
    "    negative_words_prepared = list(map(lambda x: x.strip(), f.readlines()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:35:23.947778Z",
     "start_time": "2023-11-05T09:35:23.944108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "all_toxic_words = set(toxic_words_prepared).union(set(negative_words_prepared))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:36:15.883886Z",
     "start_time": "2023-11-05T09:36:15.868169Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "SENTENCE 1/10\n",
      "----------\n",
      "Original sentence: No, I've been trying to find Olivia's killer, busting my ass--\n",
      "Original toxicity: 0.99812406\n",
      "----------\n",
      "Result sentence: No, I've been trying to find Olivia's killer, busting my ass--\n",
      "Result toxicity: 0.99812406\n",
      "----------\n",
      "Target sentence: no, I'm trying to find Olivia's killer.\n",
      "Target toxicity: 0.002062821\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 2/10\n",
      "----------\n",
      "Original sentence: what the fuck are you doing?\n",
      "Original toxicity: 0.9995204\n",
      "----------\n",
      "Result sentence: what the are you doing?\n",
      "Result toxicity: 7.6862074e-05\n",
      "----------\n",
      "Target sentence: What are you doing?\n",
      "Target toxicity: 4.8589915e-05\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 3/10\n",
      "----------\n",
      "Original sentence: Boy, I froze my ass last night!\n",
      "Original toxicity: 0.9973277\n",
      "----------\n",
      "Result sentence: Boy, I my last night!\n",
      "Result toxicity: 5.1524403e-05\n",
      "----------\n",
      "Target sentence: I almost froze last night!\n",
      "Target toxicity: 7.1020346e-05\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 4/10\n",
      "----------\n",
      "Original sentence: Why should Andorin, a Wyan of royal birth (that's what it amounted to, after all) have to work with this parvenu, this near-psychotic paranoid?\n",
      "Original toxicity: 0.66827655\n",
      "----------\n",
      "Result sentence: Why should Andorin, a Wyan of royal birth (that's what it amounted to, after all) have to work with this parvenu, this near-psychotic paranoid?\n",
      "Result toxicity: 0.66827655\n",
      "----------\n",
      "Target sentence: why must Andorin, Wyan of the royal family (that was, after all, equalled) to work with that patronym, with almost a psychotic paranoia?\n",
      "Target toxicity: 0.016706787\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 5/10\n",
      "----------\n",
      "Original sentence: It's not fucking safe here!\n",
      "Original toxicity: 0.9728687\n",
      "----------\n",
      "Result sentence: It's not safe here!\n",
      "Result toxicity: 4.2988824e-05\n",
      "----------\n",
      "Target sentence: it's not safe here!\n",
      "Target toxicity: 4.6624842e-05\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 6/10\n",
      "----------\n",
      "Original sentence: Hey, what do you fucking need?\n",
      "Original toxicity: 0.99177754\n",
      "----------\n",
      "Result sentence: Hey, what do you need?\n",
      "Result toxicity: 4.4449924e-05\n",
      "----------\n",
      "Target sentence: hey, what do you want?\n",
      "Target toxicity: 7.216999e-05\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 7/10\n",
      "----------\n",
      "Original sentence: I'm sorry Michael, but the old lady has given me hell.\n",
      "Original toxicity: 0.9002702\n",
      "----------\n",
      "Result sentence: I'm Michael, but the old lady has given me hell.\n",
      "Result toxicity: 0.94357646\n",
      "----------\n",
      "Target sentence: Sorry about that, Michael. We had this old lady here, went crazy in my place.\n",
      "Target toxicity: 0.005069314\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 8/10\n",
      "----------\n",
      "Original sentence: time to study my fists to strike your face\n",
      "Original toxicity: 0.97898155\n",
      "----------\n",
      "Result sentence: time to study my fists to your face\n",
      "Result toxicity: 0.039898794\n",
      "----------\n",
      "Target sentence: Time for you to study my fists,\n",
      "Target toxicity: 0.002333406\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 9/10\n",
      "----------\n",
      "Original sentence: She makes you feel like the girl in fat camp who got caught eating her toothpaste.\n",
      "Original toxicity: 0.9766772\n",
      "----------\n",
      "Result sentence: She makes you feel like the girl in camp who got caught eating her toothpaste.\n",
      "Result toxicity: 0.0029241436\n",
      "----------\n",
      "Target sentence: it makes you feel like a fatty on a skinny camp that was caught in eating toothpaste.\n",
      "Target toxicity: 0.06480776\n",
      "----------\n",
      "------------------------------\n",
      "SENTENCE 10/10\n",
      "----------\n",
      "Original sentence: Not that I don't have you by the balls for trying to stick me in the doorway.\n",
      "Original toxicity: 0.99690646\n",
      "----------\n",
      "Result sentence: Not that I don't have you by the for trying to stick me in the doorway.\n",
      "Result toxicity: 0.0022228516\n",
      "----------\n",
      "Target sentence: not that I don't have enough for you to welcome me through the door.\n",
      "Target toxicity: 5.692072e-05\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_remove_model(10, all_toxic_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T09:40:47.866366Z",
     "start_time": "2023-11-05T09:40:47.424751Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the dict model is more accurate and successfully detoxifies a lot more sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Making custom Seq2Seq model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (h, c) = self.lstm(x)\n",
    "\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x = self.embedding(x)\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y, teacher_forcing_ratio=0.5):\n",
    "        batch_size = x.shape[1]\n",
    "        target_len = y.shape[0]\n",
    "        target_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_output, (encoder_h, encoder_c) = self.encoder(x)\n",
    "\n",
    "        decoder_input = y\n",
    "        h, c = encoder_h, encoder_c\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, (h, c) = self.decoder(decoder_input, h, c)\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(2)\n",
    "\n",
    "            decoder_input = y[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:52:28.936819Z",
     "start_time": "2023-11-05T12:52:28.932327Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=50):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        toxic_sent = row['toxic']\n",
    "        normal_sent = row['normal']\n",
    "\n",
    "        toxic_sent = self.tokenizer(toxic_sent, return_tensors='pt', padding='max_length', truncation=True,\n",
    "                                    max_length=self.max_len)\n",
    "        normal_sent = self.tokenizer(normal_sent, return_tensors='pt', padding='max_length', truncation=True,\n",
    "                                     max_length=self.max_len)\n",
    "\n",
    "        return {\n",
    "            'toxic_sent': toxic_sent,\n",
    "            'normal_sent': normal_sent\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:52:29.126713Z",
     "start_time": "2023-11-05T12:52:29.116196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(CLF_PATH)\n",
    "\n",
    "dataset = ToxicDataset(df.data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:52:29.949578Z",
     "start_time": "2023-11-05T12:52:29.287941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "model = Seq2Seq(\n",
    "    encoder=Encoder(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.5),\n",
    "    decoder=Decoder(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.5),\n",
    "    device=DEVICE\n",
    ").to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:52:30.138336Z",
     "start_time": "2023-11-05T12:52:29.952736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:52:30.144656Z",
     "start_time": "2023-11-05T12:52:30.142189Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = batch['toxic_sent']['input_ids'].to(DEVICE).squeeze(1)\n",
    "        y = batch['normal_sent']['input_ids'].to(DEVICE).squeeze(1)\n",
    "\n",
    "        output = model(x, y)\n",
    "\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        y = y[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch}/{epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No results here :("
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
